{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we will take a look at a dataset of client reviews of Food Panda restaurants in Hong Kong. It contains the the reviewers' rating to restaurants (referred to as stores in the data). Subsequently, we will build a recommendation system using matrix factorization method. In particular, we will training a model with PyTorch based on the existing reviewer's rating to each store and break down into reviewer embeddings and store embeddings, containing latent factors for each reviewer/store. Hopefully, these embeddings are able to capture the hidden pattern/features of the reviewer/store, which can be used to predict the reviewer's rating towards a restuarant that currently has no rating from the same reviewer. \n",
    "\n",
    "Source: https://www.kaggle.com/datasets/bwandowando/hongkong-food-panda-restaurant-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreId</th>\n",
       "      <th>uuid</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>text</th>\n",
       "      <th>isAnonymous</th>\n",
       "      <th>reviewerId</th>\n",
       "      <th>replies</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>isLiked</th>\n",
       "      <th>overall</th>\n",
       "      <th>restaurant_food</th>\n",
       "      <th>rider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z7km</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>2023-05-28T09:09:59Z</td>\n",
       "      <td>2023-05-28T09:09:59Z</td>\n",
       "      <td>å‘³é“è¼ƒå’¸, è€Œä¸”ç¶“å¸¸æ¯”æ¼é‡</td>\n",
       "      <td>False</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2nn</td>\n",
       "      <td>00017c85-e20e-4d9c-8d52-c224bf2b7d6f</td>\n",
       "      <td>2023-04-30T05:02:30Z</td>\n",
       "      <td>2023-04-30T05:02:30Z</td>\n",
       "      <td>å‡ºå“å‘³é“å¯¦åœ¨ä¸éŒ¯ï¼Œéå¸¸æ¬£è³å¤§å»šçš„çƒ¹é£ª\\nä¸éæ¨“é¢æœå‹™ç¦®è²Œå¯¦åœ¨å¤ªå·®äº†ï¼Œå•å¤šä¸€å¥éƒ½ä¸è€ç…©ï¼Œæ‡‰è©²ç”Ÿ...</td>\n",
       "      <td>False</td>\n",
       "      <td>00017c85-e20e-4d9c-8d52-c224bf2b7d6f</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v9tr</td>\n",
       "      <td>00026cb8-fffc-4feb-8753-1c786f71691c</td>\n",
       "      <td>2023-07-09T13:44:31Z</td>\n",
       "      <td>2023-07-09T13:44:31Z</td>\n",
       "      <td>Super quality and authentic.</td>\n",
       "      <td>False</td>\n",
       "      <td>hk2udxt3</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2qz</td>\n",
       "      <td>0002b618-188d-4598-a4a3-cdc4f6557f35</td>\n",
       "      <td>2024-02-21T04:51:06Z</td>\n",
       "      <td>2024-02-21T04:51:06Z</td>\n",
       "      <td>å¥½å‘³ï¼ï¼ï¼</td>\n",
       "      <td>False</td>\n",
       "      <td>hkbqx1ei</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u3q3</td>\n",
       "      <td>0004a723-4885-4d3e-b942-b5d3f3d02326</td>\n",
       "      <td>2023-12-25T20:16:25Z</td>\n",
       "      <td>2023-12-25T20:16:25Z</td>\n",
       "      <td>è…¸ç²‰å¥½ç¡¬ï¼Œä¸éç…é¤ƒå¥½é£Ÿ</td>\n",
       "      <td>False</td>\n",
       "      <td>hkkofhli</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v4vf</td>\n",
       "      <td>00053eea-ed62-4e9d-9537-0dfa721f23b9</td>\n",
       "      <td>2023-06-28T05:45:13Z</td>\n",
       "      <td>2023-06-28T05:45:13Z</td>\n",
       "      <td>ç‰›è‚‰å¾ˆå¤šç­‹ï¼Œ å¤§éƒ¨åˆ†éƒ½ä¸èƒ½åƒ åäº†å‡ºä¾†ï¼Œ è¶Šä¾†è¶Šé€€æ­¥</td>\n",
       "      <td>False</td>\n",
       "      <td>hk6x2kmp</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h6ag</td>\n",
       "      <td>00070043-36ca-4087-903c-dd49cbb6fd65</td>\n",
       "      <td>2024-01-25T01:47:06Z</td>\n",
       "      <td>2024-01-25T01:47:06Z</td>\n",
       "      <td>ç²¥å¥½ç¶¿ï¼Œå¥½å‘³ï¼Œä¸éå•²æ–™éº»éº»å“‹ã€‚çš®è›‹ç˜¦è‚‰ç²¥å•²çš®è›‹åŠ åŸ‹å””çŸ¥æœ‰å†‡åŠéš»ï¼Œç˜¦è‚‰å¤ªé¹¹ã€‚</td>\n",
       "      <td>False</td>\n",
       "      <td>hkgwy1kr</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>w8ie</td>\n",
       "      <td>0008130b-f90d-4ee9-912e-efbfc70bd082</td>\n",
       "      <td>2023-08-19T06:06:03Z</td>\n",
       "      <td>2023-08-19T06:06:03Z</td>\n",
       "      <td>å¥½é£Ÿ</td>\n",
       "      <td>False</td>\n",
       "      <td>hk0huoxn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q301</td>\n",
       "      <td>00089793-4fb7-4535-992d-b6964ef7a680</td>\n",
       "      <td>2023-08-30T04:56:30Z</td>\n",
       "      <td>2023-08-30T04:56:30Z</td>\n",
       "      <td>çœŸçš„ä¸éŒ¯ğŸ‘</td>\n",
       "      <td>False</td>\n",
       "      <td>g0ssf7lj</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a8or</td>\n",
       "      <td>000da2da-18db-4df0-ae5a-017609993c61</td>\n",
       "      <td>2023-09-21T03:59:45Z</td>\n",
       "      <td>2023-09-21T03:59:45Z</td>\n",
       "      <td>ä¸Šæ¬¡èŠ«èŒœæ¹¯é£¯è¶³æ–™ï¼Œä»Šæ¬¡ç²¥å¥½é£Ÿï¼Œç‚¸ä¸¡è…¸è„†åœåœåˆç†±è¾£è¾£ï¼Œæ»¿æ„</td>\n",
       "      <td>False</td>\n",
       "      <td>hkgqh8ew</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tznf</td>\n",
       "      <td>000dab15-18ce-4d98-893f-4ec821c7afdb</td>\n",
       "      <td>2023-08-27T14:36:26Z</td>\n",
       "      <td>2023-08-27T14:36:26Z</td>\n",
       "      <td>yummy</td>\n",
       "      <td>False</td>\n",
       "      <td>hk81xahj</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>j8pt</td>\n",
       "      <td>000ff36f-0d50-40c6-8eb7-3ae80c7f5d8a</td>\n",
       "      <td>2023-04-05T14:12:30Z</td>\n",
       "      <td>2023-04-05T14:12:30Z</td>\n",
       "      <td>æ˜æ˜å«å’—ç‰›æ‰’æ„ç²‰ï¼Œæœ€å¾Œè®Šå’—é£¯ï¼Œsad</td>\n",
       "      <td>False</td>\n",
       "      <td>hkrvh5ec</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xw6y</td>\n",
       "      <td>00109778-bc12-4fb4-b0c9-201b757a5b45</td>\n",
       "      <td>2023-05-15T13:35:08Z</td>\n",
       "      <td>2023-05-15T13:35:08Z</td>\n",
       "      <td>èŒ„å­å¤ªæ²¹ï¼Œå’Œéƒ¨é£Ÿç‰©çš„ä¹‹ç„¶å‘³å¤ªæ¿ƒï¼Œå¦å‰‡æ•´é«”ä¸éŒ¯</td>\n",
       "      <td>False</td>\n",
       "      <td>hkzw04fx</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>v9tj</td>\n",
       "      <td>0013eb6f-111d-4aad-91e6-5b9bc4769e97</td>\n",
       "      <td>2023-05-04T12:05:53Z</td>\n",
       "      <td>2023-05-04T12:05:53Z</td>\n",
       "      <td>ä»½é‡å°‘</td>\n",
       "      <td>False</td>\n",
       "      <td>hkh6799n</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>r567</td>\n",
       "      <td>001461c5-a72d-4e70-acf1-571c07233a78</td>\n",
       "      <td>2023-09-17T05:39:57Z</td>\n",
       "      <td>2023-09-17T05:39:57Z</td>\n",
       "      <td>æ¼å–®\\næ‰“å–®å€‹å€‹ä»²è¦å¾ˆç„¡ç¦®ï¼Œå¥½ä¼¼ä¿‚æˆ‘éŒ¯å””é—œä½¢äº‹å’</td>\n",
       "      <td>False</td>\n",
       "      <td>001461c5-a72d-4e70-acf1-571c07233a78</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>v3mh</td>\n",
       "      <td>0015fb07-5339-45ca-bad2-38f030f706d8</td>\n",
       "      <td>2024-01-11T06:50:14Z</td>\n",
       "      <td>2024-01-11T06:50:14Z</td>\n",
       "      <td>åŒ…è£å¥½, ä»½é‡å¤ </td>\n",
       "      <td>False</td>\n",
       "      <td>hktze1el</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tbrh</td>\n",
       "      <td>00166372-0968-4f40-b244-1649f412b667</td>\n",
       "      <td>2023-06-13T10:25:37Z</td>\n",
       "      <td>2023-06-13T10:25:37Z</td>\n",
       "      <td>æˆ‘ä¸æœƒç”¨40å…ƒè²·ä¸€ç›’ä¸­è¯æ²™å¾‹(å¤§é€£èœï¼‰ğŸ¤·ğŸ»â€â™€ï¸</td>\n",
       "      <td>False</td>\n",
       "      <td>hkjhwwe0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>j8pt</td>\n",
       "      <td>00170224-8337-42cd-aae5-c71cac584729</td>\n",
       "      <td>2023-09-18T12:43:04Z</td>\n",
       "      <td>2023-09-18T12:43:04Z</td>\n",
       "      <td>å¥½å¥½é£Ÿï¼Œç”¨å¿ƒè£½ä½œ</td>\n",
       "      <td>False</td>\n",
       "      <td>hkh9sx12</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ddys</td>\n",
       "      <td>0018c418-1cf2-4d83-bf26-47178ff64918</td>\n",
       "      <td>2023-05-18T14:57:02Z</td>\n",
       "      <td>2023-05-18T14:57:02Z</td>\n",
       "      <td>ç‚’é£¯ç‚’ç²‰éƒ½ä¸éŒ¯</td>\n",
       "      <td>False</td>\n",
       "      <td>hk5qkim6</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>s5jy</td>\n",
       "      <td>00194aff-9d0f-45ef-b012-c41aac008d79</td>\n",
       "      <td>2023-11-29T05:50:43Z</td>\n",
       "      <td>2023-11-29T05:50:43Z</td>\n",
       "      <td>è’œè¾£æ±å¥½é£ŸğŸ‘ğŸ»</td>\n",
       "      <td>False</td>\n",
       "      <td>x8qvv5jh</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StoreId                                  uuid             createdAt  \\\n",
       "0     z7km  00006b05-61f8-4f5c-9244-00ba8d97bc0e  2023-05-28T09:09:59Z   \n",
       "1     v2nn  00017c85-e20e-4d9c-8d52-c224bf2b7d6f  2023-04-30T05:02:30Z   \n",
       "2     v9tr  00026cb8-fffc-4feb-8753-1c786f71691c  2023-07-09T13:44:31Z   \n",
       "3     h2qz  0002b618-188d-4598-a4a3-cdc4f6557f35  2024-02-21T04:51:06Z   \n",
       "4     u3q3  0004a723-4885-4d3e-b942-b5d3f3d02326  2023-12-25T20:16:25Z   \n",
       "5     v4vf  00053eea-ed62-4e9d-9537-0dfa721f23b9  2023-06-28T05:45:13Z   \n",
       "6     h6ag  00070043-36ca-4087-903c-dd49cbb6fd65  2024-01-25T01:47:06Z   \n",
       "7     w8ie  0008130b-f90d-4ee9-912e-efbfc70bd082  2023-08-19T06:06:03Z   \n",
       "8     q301  00089793-4fb7-4535-992d-b6964ef7a680  2023-08-30T04:56:30Z   \n",
       "9     a8or  000da2da-18db-4df0-ae5a-017609993c61  2023-09-21T03:59:45Z   \n",
       "10    tznf  000dab15-18ce-4d98-893f-4ec821c7afdb  2023-08-27T14:36:26Z   \n",
       "11    j8pt  000ff36f-0d50-40c6-8eb7-3ae80c7f5d8a  2023-04-05T14:12:30Z   \n",
       "12    xw6y  00109778-bc12-4fb4-b0c9-201b757a5b45  2023-05-15T13:35:08Z   \n",
       "13    v9tj  0013eb6f-111d-4aad-91e6-5b9bc4769e97  2023-05-04T12:05:53Z   \n",
       "14    r567  001461c5-a72d-4e70-acf1-571c07233a78  2023-09-17T05:39:57Z   \n",
       "15    v3mh  0015fb07-5339-45ca-bad2-38f030f706d8  2024-01-11T06:50:14Z   \n",
       "16    tbrh  00166372-0968-4f40-b244-1649f412b667  2023-06-13T10:25:37Z   \n",
       "17    j8pt  00170224-8337-42cd-aae5-c71cac584729  2023-09-18T12:43:04Z   \n",
       "18    ddys  0018c418-1cf2-4d83-bf26-47178ff64918  2023-05-18T14:57:02Z   \n",
       "19    s5jy  00194aff-9d0f-45ef-b012-c41aac008d79  2023-11-29T05:50:43Z   \n",
       "\n",
       "               updatedAt                                               text  \\\n",
       "0   2023-05-28T09:09:59Z                                      å‘³é“è¼ƒå’¸, è€Œä¸”ç¶“å¸¸æ¯”æ¼é‡   \n",
       "1   2023-04-30T05:02:30Z  å‡ºå“å‘³é“å¯¦åœ¨ä¸éŒ¯ï¼Œéå¸¸æ¬£è³å¤§å»šçš„çƒ¹é£ª\\nä¸éæ¨“é¢æœå‹™ç¦®è²Œå¯¦åœ¨å¤ªå·®äº†ï¼Œå•å¤šä¸€å¥éƒ½ä¸è€ç…©ï¼Œæ‡‰è©²ç”Ÿ...   \n",
       "2   2023-07-09T13:44:31Z                     Super quality and authentic.     \n",
       "3   2024-02-21T04:51:06Z                                              å¥½å‘³ï¼ï¼ï¼   \n",
       "4   2023-12-25T20:16:25Z                                        è…¸ç²‰å¥½ç¡¬ï¼Œä¸éç…é¤ƒå¥½é£Ÿ   \n",
       "5   2023-06-28T05:45:13Z                         ç‰›è‚‰å¾ˆå¤šç­‹ï¼Œ å¤§éƒ¨åˆ†éƒ½ä¸èƒ½åƒ åäº†å‡ºä¾†ï¼Œ è¶Šä¾†è¶Šé€€æ­¥   \n",
       "6   2024-01-25T01:47:06Z              ç²¥å¥½ç¶¿ï¼Œå¥½å‘³ï¼Œä¸éå•²æ–™éº»éº»å“‹ã€‚çš®è›‹ç˜¦è‚‰ç²¥å•²çš®è›‹åŠ åŸ‹å””çŸ¥æœ‰å†‡åŠéš»ï¼Œç˜¦è‚‰å¤ªé¹¹ã€‚   \n",
       "7   2023-08-19T06:06:03Z                                                 å¥½é£Ÿ   \n",
       "8   2023-08-30T04:56:30Z                                              çœŸçš„ä¸éŒ¯ğŸ‘   \n",
       "9   2023-09-21T03:59:45Z                       ä¸Šæ¬¡èŠ«èŒœæ¹¯é£¯è¶³æ–™ï¼Œä»Šæ¬¡ç²¥å¥½é£Ÿï¼Œç‚¸ä¸¡è…¸è„†åœåœåˆç†±è¾£è¾£ï¼Œæ»¿æ„   \n",
       "10  2023-08-27T14:36:26Z                                              yummy   \n",
       "11  2023-04-05T14:12:30Z                                 æ˜æ˜å«å’—ç‰›æ‰’æ„ç²‰ï¼Œæœ€å¾Œè®Šå’—é£¯ï¼Œsad   \n",
       "12  2023-05-15T13:35:08Z                             èŒ„å­å¤ªæ²¹ï¼Œå’Œéƒ¨é£Ÿç‰©çš„ä¹‹ç„¶å‘³å¤ªæ¿ƒï¼Œå¦å‰‡æ•´é«”ä¸éŒ¯   \n",
       "13  2023-05-04T12:05:53Z                                                ä»½é‡å°‘   \n",
       "14  2023-09-17T05:39:57Z                           æ¼å–®\\næ‰“å–®å€‹å€‹ä»²è¦å¾ˆç„¡ç¦®ï¼Œå¥½ä¼¼ä¿‚æˆ‘éŒ¯å””é—œä½¢äº‹å’   \n",
       "15  2024-01-11T06:50:14Z                                           åŒ…è£å¥½, ä»½é‡å¤    \n",
       "16  2023-06-13T10:25:37Z                           æˆ‘ä¸æœƒç”¨40å…ƒè²·ä¸€ç›’ä¸­è¯æ²™å¾‹(å¤§é€£èœï¼‰ğŸ¤·ğŸ»â€â™€ï¸   \n",
       "17  2023-09-18T12:43:04Z                                           å¥½å¥½é£Ÿï¼Œç”¨å¿ƒè£½ä½œ   \n",
       "18  2023-05-18T14:57:02Z                                            ç‚’é£¯ç‚’ç²‰éƒ½ä¸éŒ¯   \n",
       "19  2023-11-29T05:50:43Z                                            è’œè¾£æ±å¥½é£ŸğŸ‘ğŸ»   \n",
       "\n",
       "    isAnonymous                            reviewerId replies  likeCount  \\\n",
       "0         False  00006b05-61f8-4f5c-9244-00ba8d97bc0e      []          1   \n",
       "1         False  00017c85-e20e-4d9c-8d52-c224bf2b7d6f      []          0   \n",
       "2         False                              hk2udxt3      []          0   \n",
       "3         False                              hkbqx1ei      []          0   \n",
       "4         False                              hkkofhli      []          0   \n",
       "5         False                              hk6x2kmp      []          0   \n",
       "6         False                              hkgwy1kr      []          0   \n",
       "7         False                              hk0huoxn      []          0   \n",
       "8         False                              g0ssf7lj      []          0   \n",
       "9         False                              hkgqh8ew      []          0   \n",
       "10        False                              hk81xahj      []          0   \n",
       "11        False                              hkrvh5ec      []          0   \n",
       "12        False                              hkzw04fx      []          0   \n",
       "13        False                              hkh6799n      []          0   \n",
       "14        False  001461c5-a72d-4e70-acf1-571c07233a78      []          0   \n",
       "15        False                              hktze1el      []          0   \n",
       "16        False                              hkjhwwe0      []          1   \n",
       "17        False                              hkh9sx12      []          0   \n",
       "18        False                              hk5qkim6      []          0   \n",
       "19        False                              x8qvv5jh      []          0   \n",
       "\n",
       "    isLiked  overall  restaurant_food  rider  \n",
       "0     False        2                2    NaN  \n",
       "1     False        1                1    NaN  \n",
       "2     False        5                5    NaN  \n",
       "3     False        5                5    5.0  \n",
       "4     False        3                3    NaN  \n",
       "5     False        2                2    NaN  \n",
       "6     False        4                4    NaN  \n",
       "7     False        5                5    5.0  \n",
       "8     False        4                4    4.0  \n",
       "9     False        5                5    NaN  \n",
       "10    False        5                5    5.0  \n",
       "11    False        1                1    2.0  \n",
       "12    False        4                4    5.0  \n",
       "13    False        3                3    2.0  \n",
       "14    False        1                1    NaN  \n",
       "15    False        5                5    NaN  \n",
       "16    False        2                2    NaN  \n",
       "17    False        5                5    NaN  \n",
       "18    False        4                4    NaN  \n",
       "19    False        5                5    NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "path = r'.\\Data\\hk_hong_kong_reviews.csv'\n",
    "reviews_df = pd.read_csv(path, header = 0)\n",
    "reviews_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv parsing looks fine in Vs code. However, if such data is read in other platform (e.g., Databricks), some may find that the data is distorted by \"\\n\" in the text column. Also, it is found that the text column containing line breaks are all quoted with double quototation marks (\"<text\\>\"), and there are also cases where double quoted text do not include a line break as well as cases where there are double double-quoted words (e.g., \"\"<word\\>\"\") within the text columns. These format may confuse the data parsing when converting into dataframe. \n",
    "\n",
    "One option is to drop those rows using dropna(). However, in order to preserve as much data as possible, we can use regular expression to clean the data. In this case, the following code can be used to fix the above issues if encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "with open(r'Data/hk_hong_kong_reviews.csv',\"r\") as txt:\n",
    "    t = txt.readlines()\n",
    "text = \"\"\n",
    "for line in t:\n",
    "    text += line\n",
    "\n",
    "text = text.replace('\"\"','')\n",
    "\n",
    "pattern = '\\\"([^\"]*|\\n)*\\\"'\n",
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "text_idx = []\n",
    "for match in matches:\n",
    "    if text.find(\"\\n\",match.start(),match.end()) != -1:\n",
    "        text_idx.append((match.start(),match.end()))\n",
    "\n",
    "last_idx = 0\n",
    "cleaned_data_str = \"\"\n",
    "for idx in text_idx:\n",
    "    cleaned_data_str+=text[last_idx:idx[0]]\n",
    "    revised_text = re.sub(\"\\n\",\" \", text[idx[0]:idx[1]])\n",
    "    cleaned_data_str += revised_text\n",
    "    last_idx = idx[1]\n",
    "cleaned_data_str += text[last_idx:len(text)]\n",
    "\n",
    "with open(r'Data/hk_hong_kong_reviews(clean).csv',\"w\") as f_write:\n",
    "    f_write.write(cleaned_data_str)\n",
    "\n",
    "data = pd.read_csv(StringIO(cleaned_data_str), header=0, quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check whether the cleaned data still have the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreId</th>\n",
       "      <th>uuid</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>text</th>\n",
       "      <th>isAnonymous</th>\n",
       "      <th>reviewerId</th>\n",
       "      <th>replies</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>isLiked</th>\n",
       "      <th>overall</th>\n",
       "      <th>restaurant_food</th>\n",
       "      <th>rider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z7km</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>2023-05-28T09:09:59Z</td>\n",
       "      <td>2023-05-28T09:09:59Z</td>\n",
       "      <td>å‘³é“è¼ƒå’¸, è€Œä¸”ç¶“å¸¸æ¯”æ¼é‡</td>\n",
       "      <td>False</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2nn</td>\n",
       "      <td>00017c85-e20e-4d9c-8d52-c224bf2b7d6f</td>\n",
       "      <td>2023-04-30T05:02:30Z</td>\n",
       "      <td>2023-04-30T05:02:30Z</td>\n",
       "      <td>å‡ºå“å‘³é“å¯¦åœ¨ä¸éŒ¯ï¼Œéå¸¸æ¬£è³å¤§å»šçš„çƒ¹é£ª ä¸éæ¨“é¢æœå‹™ç¦®è²Œå¯¦åœ¨å¤ªå·®äº†ï¼Œå•å¤šä¸€å¥éƒ½ä¸è€ç…©ï¼Œæ‡‰è©²ç”Ÿæ„...</td>\n",
       "      <td>False</td>\n",
       "      <td>00017c85-e20e-4d9c-8d52-c224bf2b7d6f</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v9tr</td>\n",
       "      <td>00026cb8-fffc-4feb-8753-1c786f71691c</td>\n",
       "      <td>2023-07-09T13:44:31Z</td>\n",
       "      <td>2023-07-09T13:44:31Z</td>\n",
       "      <td>Super quality and authentic.</td>\n",
       "      <td>False</td>\n",
       "      <td>hk2udxt3</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2qz</td>\n",
       "      <td>0002b618-188d-4598-a4a3-cdc4f6557f35</td>\n",
       "      <td>2024-02-21T04:51:06Z</td>\n",
       "      <td>2024-02-21T04:51:06Z</td>\n",
       "      <td>å¥½å‘³ï¼ï¼ï¼</td>\n",
       "      <td>False</td>\n",
       "      <td>hkbqx1ei</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u3q3</td>\n",
       "      <td>0004a723-4885-4d3e-b942-b5d3f3d02326</td>\n",
       "      <td>2023-12-25T20:16:25Z</td>\n",
       "      <td>2023-12-25T20:16:25Z</td>\n",
       "      <td>è…¸ç²‰å¥½ç¡¬ï¼Œä¸éç…é¤ƒå¥½é£Ÿ</td>\n",
       "      <td>False</td>\n",
       "      <td>hkkofhli</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StoreId                                  uuid             createdAt  \\\n",
       "0    z7km  00006b05-61f8-4f5c-9244-00ba8d97bc0e  2023-05-28T09:09:59Z   \n",
       "1    v2nn  00017c85-e20e-4d9c-8d52-c224bf2b7d6f  2023-04-30T05:02:30Z   \n",
       "2    v9tr  00026cb8-fffc-4feb-8753-1c786f71691c  2023-07-09T13:44:31Z   \n",
       "3    h2qz  0002b618-188d-4598-a4a3-cdc4f6557f35  2024-02-21T04:51:06Z   \n",
       "4    u3q3  0004a723-4885-4d3e-b942-b5d3f3d02326  2023-12-25T20:16:25Z   \n",
       "\n",
       "              updatedAt                                               text  \\\n",
       "0  2023-05-28T09:09:59Z                                      å‘³é“è¼ƒå’¸, è€Œä¸”ç¶“å¸¸æ¯”æ¼é‡   \n",
       "1  2023-04-30T05:02:30Z  å‡ºå“å‘³é“å¯¦åœ¨ä¸éŒ¯ï¼Œéå¸¸æ¬£è³å¤§å»šçš„çƒ¹é£ª ä¸éæ¨“é¢æœå‹™ç¦®è²Œå¯¦åœ¨å¤ªå·®äº†ï¼Œå•å¤šä¸€å¥éƒ½ä¸è€ç…©ï¼Œæ‡‰è©²ç”Ÿæ„...   \n",
       "2  2023-07-09T13:44:31Z                     Super quality and authentic.     \n",
       "3  2024-02-21T04:51:06Z                                              å¥½å‘³ï¼ï¼ï¼   \n",
       "4  2023-12-25T20:16:25Z                                        è…¸ç²‰å¥½ç¡¬ï¼Œä¸éç…é¤ƒå¥½é£Ÿ   \n",
       "\n",
       "   isAnonymous                            reviewerId replies  likeCount  \\\n",
       "0        False  00006b05-61f8-4f5c-9244-00ba8d97bc0e      []          1   \n",
       "1        False  00017c85-e20e-4d9c-8d52-c224bf2b7d6f      []          0   \n",
       "2        False                              hk2udxt3      []          0   \n",
       "3        False                              hkbqx1ei      []          0   \n",
       "4        False                              hkkofhli      []          0   \n",
       "\n",
       "   isLiked  overall  restaurant_food  rider  \n",
       "0    False        2                2    NaN  \n",
       "1    False        1                1    NaN  \n",
       "2    False        5                5    NaN  \n",
       "3    False        5                5    5.0  \n",
       "4    False        3                3    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(r'Data/hk_hong_kong_reviews(clean).csv', header=0, quotechar='\"')\n",
    "reviews_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50548 entries, 0 to 50547\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   StoreId          50548 non-null  object \n",
      " 1   uuid             50548 non-null  object \n",
      " 2   createdAt        50548 non-null  object \n",
      " 3   updatedAt        50548 non-null  object \n",
      " 4   text             50544 non-null  object \n",
      " 5   isAnonymous      50548 non-null  bool   \n",
      " 6   reviewerId       50548 non-null  object \n",
      " 7   replies          50548 non-null  object \n",
      " 8   likeCount        50548 non-null  int64  \n",
      " 9   isLiked          50548 non-null  bool   \n",
      " 10  overall          50548 non-null  int64  \n",
      " 11  restaurant_food  50548 non-null  int64  \n",
      " 12  rider            17179 non-null  float64\n",
      "dtypes: bool(2), float64(1), int64(3), object(7)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated records: 0\n",
      "Number of records with missing rating: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplication\n",
    "duplicated_count = reviews_df.duplicated().sum()\n",
    "\n",
    "# Check for missing rating\n",
    "missing_rating_count = len(reviews_df[reviews_df['overall'].isna()])\n",
    "\n",
    "print(f\"Number of duplicated records: {duplicated_count}\")\n",
    "print(f\"Number of records with missing rating: {missing_rating_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will split the data into training and test set. For time-series data, we will normally split the data based on the time that the records are created. In this analysis, we will take the first 80% of data as training set and remaining 20% as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date ranges from 2023-03-27T13:41:35Z to 2024-03-27T17:50:11Z\n"
     ]
    }
   ],
   "source": [
    "# Sort by time\n",
    "reviews_df = reviews_df.sort_values(by='createdAt', ascending=True)\n",
    "\n",
    "# Check date range\n",
    "min_date = reviews_df['createdAt'].min()\n",
    "max_date = reviews_df['createdAt'].max()\n",
    "\n",
    "print(f\"Date ranges from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['reviewerId', 'storeId', 'overall'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_20616\\859120231.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.rename(columns={'StoreId':'storeId'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37004, 9252)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select needed columns and split data into training and test set\n",
    "data = reviews_df[['reviewerId', 'StoreId', 'overall']]\n",
    "data.rename(columns={'StoreId':'storeId'}, inplace=True)\n",
    "print(data.columns)\n",
    "\n",
    "# Drop duplicates, blank rating\n",
    "data = data.drop_duplicates().dropna()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(len(data)*0.8)\n",
    "train_data, test_data = data[0:train_size], data[train_size:]\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of training the model in PyTorch, we need to encode the reviewerId and StoreId with continous Id. The encoded id will serve as the index in the reviewer/store embeddings. We can create a dictionary so that we can convert the encoded id back to original id afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewer ID: [  520 26247 23528 ...  8712  5207 18748]\n",
      "Store ID: [1002 4133 4657 ... 4459 1080 3226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_20616\\1362070608.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['reviewerId'] = train_data['reviewerId'].map(reviewerid2idx)\n",
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_20616\\1362070608.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['reviewerId'] = test_data['reviewerId'].map(lambda x: reviewerid2idx.get(x,-1)) # -1 for users not in training\n",
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_20616\\1362070608.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['storeId'] = train_data['storeId'].map(storeid2idx)\n"
     ]
    }
   ],
   "source": [
    "# Encoding reviewerId with continous Id\n",
    "train_reviewerId = np.sort(np.unique(train_data.reviewerId.values))\n",
    "num_reviewers = len(train_reviewerId)\n",
    "reviewerid2idx = {o:i for i,o in enumerate(train_reviewerId)}\n",
    "train_data['reviewerId'] = train_data['reviewerId'].map(reviewerid2idx)\n",
    "test_data['reviewerId'] = test_data['reviewerId'].map(lambda x: reviewerid2idx.get(x,-1)) # -1 for users not in training\n",
    "test_data = test_data[test_data['reviewerId'] >= 0].copy()\n",
    "\n",
    "# Encoding StoreId with continous Id\n",
    "train_storeId = np.sort(np.unique(train_data.storeId.values))\n",
    "num_stores = len(train_storeId)\n",
    "storeid2idx = {o:i for i,o in enumerate(train_storeId)}\n",
    "train_data['storeId'] = train_data['storeId'].map(storeid2idx)\n",
    "test_data['storeId'] = test_data['storeId'].map(lambda x: storeid2idx.get(x,-1)) # -1 for users not in training\n",
    "test_data = test_data[test_data['storeId'] >= 0].copy()\n",
    "\n",
    "print(f\"Reviewer ID: {train_data['reviewerId'].unique()}\")\n",
    "print(f\"Store ID: {train_data['storeId'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we will build 2 versions of model, one without bias term and one with bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1: No bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define the model archietecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys(nn.Module):\n",
    "    def __init__(self, num_reviewers, num_stores, emb_size=100):\n",
    "        super().__init__()\n",
    "        self.reviewers_emb = nn.Embedding(num_reviewers, emb_size)\n",
    "        self.stores_emb = nn.Embedding(num_stores, emb_size)\n",
    "        # reinitializing weights with a smaller value as it is found that this model works better if start with smaller values\n",
    "        self.reviewers_emb.weight.data.uniform_(0,0.05)\n",
    "        self.stores_emb.weight.data.uniform_(0,0.05)\n",
    "    \n",
    "    def forward(self, reviewer_id, store_id):\n",
    "        u = self.reviewers_emb(reviewer_id) # This tells which indices of the reviewer embedding to extract\n",
    "        v = self.stores_emb(store_id) # This tells which indices of the store embedding to extract\n",
    "        return (u*v).sum(dim=1) # this is the dot product (first get element-wise multiplication then sum it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are not using data loaders because our data fits well in memory\n",
    "\n",
    "def train_recsys(model, train:pd.DataFrame, test:pd.DataFrame, epochs=10, lr=0.01, weight_decay=0.0, device='cpu'): # Weight decay is effectively the lambda in regularization\n",
    "\n",
    "    # Training loop    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        reviewers = torch.LongTensor(train.reviewerId.values).to(device)\n",
    "        stores = torch.LongTensor(train.storeId.values).to(device)\n",
    "        ratings = torch.FloatTensor(train.overall.values).to(device)\n",
    "\n",
    "        y_pred = model(reviewers, stores)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train_loss = F.l1_loss(y_pred, ratings) # We are using L1 loss (Mean absolute errors) as this will show errors in the same unit as the data which is easier for us to conceptualize how well our model performs.\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1} | train loss {train_loss.item():.3f}\")\n",
    "        \n",
    "    # Test loop\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        reviewers = torch.LongTensor(train.reviewerId.values).to(device)\n",
    "        stores = torch.LongTensor(train.storeId.values).to(device)\n",
    "        ratings = torch.FloatTensor(train.overall.values).to(device)\n",
    "        test_pred = model(reviewers, stores)\n",
    "        test_loss = F.l1_loss(test_pred, ratings)\n",
    "        \n",
    "        print(f\"test loss {test_loss.item():.3f}\")\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss 2.022\n",
      "Epoch 20 | train loss 0.518\n",
      "Epoch 30 | train loss 0.402\n",
      "Epoch 40 | train loss 0.395\n",
      "Epoch 50 | train loss 0.392\n",
      "Epoch 60 | train loss 0.390\n",
      "Epoch 70 | train loss 0.387\n",
      "Epoch 80 | train loss 0.385\n",
      "Epoch 90 | train loss 0.383\n",
      "Epoch 100 | train loss 0.381\n",
      "test loss 0.361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3808, grad_fn=<MeanBackward0>), tensor(0.3612))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_reviewers = len(train_data['reviewerId'].unique())\n",
    "num_stores = len(train_data['storeId'].unique())\n",
    "\n",
    "recsys_model = RecSys(num_reviewers, num_stores, emb_size=100)\n",
    "train_recsys(recsys_model, train_data, test_data, epochs=100, lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2: With bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, emb_size=100):\n",
    "        super().__init__()\n",
    "        self.reviewers_emb = nn.Embedding(num_reviewers, emb_size)\n",
    "        self.stores_emb = nn.Embedding(num_stores, emb_size)       \n",
    "        self.reviewers_bias = nn.Embedding(num_reviewers, 1)\n",
    "        self.stores_bias = nn.Embedding(num_stores, 1)\n",
    "        \n",
    "        # re-initializing weights with a smaller value as it is found that this model works better if start with smaller values\n",
    "        self.reviewers_emb.weight.data.uniform_(0,0.05)\n",
    "        self.stores_emb.weight.data.uniform_(0,0.05)\n",
    "        self.reviewers_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.stores_bias.weight.data.uniform_(-0.01,0.01)\n",
    "\n",
    "    def forward(self, user_id, movie_id):\n",
    "        u = self.reviewers_emb(user_id)\n",
    "        v = self.stores_emb(movie_id)\n",
    "        b_u = self.reviewers_bias(user_id).squeeze()\n",
    "        b_v = self.stores_bias(movie_id).squeeze()\n",
    "        return (u*v).sum(dim=1) + b_u +b_v # this is the dot product (first get element-wise multiplication then sum it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss 1.904\n",
      "Epoch 20 | train loss 0.411\n",
      "Epoch 30 | train loss 0.374\n",
      "Epoch 40 | train loss 0.369\n",
      "Epoch 50 | train loss 0.366\n",
      "Epoch 60 | train loss 0.364\n",
      "Epoch 70 | train loss 0.362\n",
      "Epoch 80 | train loss 0.360\n",
      "Epoch 90 | train loss 0.359\n",
      "Epoch 100 | train loss 0.357\n",
      "test loss 0.379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3575, grad_fn=<MeanBackward0>), tensor(0.3792))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "\n",
    "num_reviewers = len(train_data['reviewerId'].unique())\n",
    "num_stores = len(train_data['storeId'].unique())\n",
    "\n",
    "recsys_bias_model = RecSys_bias(num_reviewers, num_stores, emb_size=100)\n",
    "train_recsys(recsys_bias_model, train_data, test_data, epochs=100, lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the performance of model with bias is similar to without bias. Let's go for the model with bias. Now, let's vary the embedding size to see identify the optimal size of model. However, the larger the embedding size, more epochs are required to converge, so let's try 1000 epochs this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss 2.883\n",
      "Epoch 20 | train loss 2.357\n",
      "Epoch 30 | train loss 1.830\n",
      "Epoch 40 | train loss 1.278\n",
      "Epoch 50 | train loss 0.762\n",
      "Epoch 60 | train loss 0.362\n",
      "Epoch 70 | train loss 0.318\n",
      "Epoch 80 | train loss 0.306\n",
      "Epoch 90 | train loss 0.302\n",
      "Epoch 100 | train loss 0.300\n",
      "Epoch 110 | train loss 0.298\n",
      "Epoch 120 | train loss 0.297\n",
      "Epoch 130 | train loss 0.296\n",
      "Epoch 140 | train loss 0.296\n",
      "Epoch 150 | train loss 0.295\n",
      "Epoch 160 | train loss 0.295\n",
      "Epoch 170 | train loss 0.294\n",
      "Epoch 180 | train loss 0.294\n",
      "Epoch 190 | train loss 0.293\n",
      "Epoch 200 | train loss 0.293\n",
      "Epoch 210 | train loss 0.293\n",
      "Epoch 220 | train loss 0.292\n",
      "Epoch 230 | train loss 0.292\n",
      "Epoch 240 | train loss 0.291\n",
      "Epoch 250 | train loss 0.291\n",
      "Epoch 260 | train loss 0.291\n",
      "Epoch 270 | train loss 0.291\n",
      "Epoch 280 | train loss 0.290\n",
      "Epoch 290 | train loss 0.290\n",
      "Epoch 300 | train loss 0.290\n",
      "Epoch 310 | train loss 0.290\n",
      "Epoch 320 | train loss 0.290\n",
      "Epoch 330 | train loss 0.290\n",
      "Epoch 340 | train loss 0.290\n",
      "Epoch 350 | train loss 0.290\n",
      "Epoch 360 | train loss 0.290\n",
      "Epoch 370 | train loss 0.290\n",
      "Epoch 380 | train loss 0.290\n",
      "Epoch 390 | train loss 0.290\n",
      "Epoch 400 | train loss 0.289\n",
      "Epoch 410 | train loss 0.289\n",
      "Epoch 420 | train loss 0.289\n",
      "Epoch 430 | train loss 0.289\n",
      "Epoch 440 | train loss 0.289\n",
      "Epoch 450 | train loss 0.289\n",
      "Epoch 460 | train loss 0.289\n",
      "Epoch 470 | train loss 0.289\n",
      "Epoch 480 | train loss 0.289\n",
      "Epoch 490 | train loss 0.289\n",
      "Epoch 500 | train loss 0.289\n",
      "Epoch 510 | train loss 0.289\n",
      "Epoch 520 | train loss 0.289\n",
      "Epoch 530 | train loss 0.289\n",
      "Epoch 540 | train loss 0.289\n",
      "Epoch 550 | train loss 0.289\n",
      "Epoch 560 | train loss 0.289\n",
      "Epoch 570 | train loss 0.289\n",
      "Epoch 580 | train loss 0.289\n",
      "Epoch 590 | train loss 0.289\n",
      "Epoch 600 | train loss 0.289\n",
      "Epoch 610 | train loss 0.289\n",
      "Epoch 620 | train loss 0.289\n",
      "Epoch 630 | train loss 0.289\n",
      "Epoch 640 | train loss 0.289\n",
      "Epoch 650 | train loss 0.289\n",
      "Epoch 660 | train loss 0.289\n",
      "Epoch 670 | train loss 0.288\n",
      "Epoch 680 | train loss 0.288\n",
      "Epoch 690 | train loss 0.288\n",
      "Epoch 700 | train loss 0.288\n",
      "Epoch 710 | train loss 0.288\n",
      "Epoch 720 | train loss 0.288\n",
      "Epoch 730 | train loss 0.288\n",
      "Epoch 740 | train loss 0.288\n",
      "Epoch 750 | train loss 0.288\n",
      "Epoch 760 | train loss 0.288\n",
      "Epoch 770 | train loss 0.288\n",
      "Epoch 780 | train loss 0.288\n",
      "Epoch 790 | train loss 0.288\n",
      "Epoch 800 | train loss 0.288\n",
      "Epoch 810 | train loss 0.288\n",
      "Epoch 820 | train loss 0.288\n",
      "Epoch 830 | train loss 0.288\n",
      "Epoch 840 | train loss 0.288\n",
      "Epoch 850 | train loss 0.288\n",
      "Epoch 860 | train loss 0.288\n",
      "Epoch 870 | train loss 0.288\n",
      "Epoch 880 | train loss 0.288\n",
      "Epoch 890 | train loss 0.288\n",
      "Epoch 900 | train loss 0.288\n",
      "Epoch 910 | train loss 0.288\n",
      "Epoch 920 | train loss 0.288\n",
      "Epoch 930 | train loss 0.287\n",
      "Epoch 940 | train loss 0.287\n",
      "Epoch 950 | train loss 0.287\n",
      "Epoch 960 | train loss 0.287\n",
      "Epoch 970 | train loss 0.287\n",
      "Epoch 980 | train loss 0.287\n",
      "Epoch 990 | train loss 0.287\n",
      "Epoch 1000 | train loss 0.287\n",
      "test loss 0.283\n",
      "Epoch 10 | train loss 2.367\n",
      "Epoch 20 | train loss 1.197\n",
      "Epoch 30 | train loss 0.384\n",
      "Epoch 40 | train loss 0.352\n",
      "Epoch 50 | train loss 0.347\n",
      "Epoch 60 | train loss 0.345\n",
      "Epoch 70 | train loss 0.344\n",
      "Epoch 80 | train loss 0.342\n",
      "Epoch 90 | train loss 0.341\n",
      "Epoch 100 | train loss 0.340\n",
      "Epoch 110 | train loss 0.339\n",
      "Epoch 120 | train loss 0.338\n",
      "Epoch 130 | train loss 0.337\n",
      "Epoch 140 | train loss 0.337\n",
      "Epoch 150 | train loss 0.336\n",
      "Epoch 160 | train loss 0.335\n",
      "Epoch 170 | train loss 0.335\n",
      "Epoch 180 | train loss 0.335\n",
      "Epoch 190 | train loss 0.334\n",
      "Epoch 200 | train loss 0.334\n",
      "Epoch 210 | train loss 0.333\n",
      "Epoch 220 | train loss 0.333\n",
      "Epoch 230 | train loss 0.333\n",
      "Epoch 240 | train loss 0.332\n",
      "Epoch 250 | train loss 0.332\n",
      "Epoch 260 | train loss 0.332\n",
      "Epoch 270 | train loss 0.332\n",
      "Epoch 280 | train loss 0.331\n",
      "Epoch 290 | train loss 0.331\n",
      "Epoch 300 | train loss 0.331\n",
      "Epoch 310 | train loss 0.330\n",
      "Epoch 320 | train loss 0.330\n",
      "Epoch 330 | train loss 0.330\n",
      "Epoch 340 | train loss 0.330\n",
      "Epoch 350 | train loss 0.329\n",
      "Epoch 360 | train loss 0.329\n",
      "Epoch 370 | train loss 0.329\n",
      "Epoch 380 | train loss 0.329\n",
      "Epoch 390 | train loss 0.328\n",
      "Epoch 400 | train loss 0.328\n",
      "Epoch 410 | train loss 0.328\n",
      "Epoch 420 | train loss 0.328\n",
      "Epoch 430 | train loss 0.328\n",
      "Epoch 440 | train loss 0.328\n",
      "Epoch 450 | train loss 0.328\n",
      "Epoch 460 | train loss 0.328\n",
      "Epoch 470 | train loss 0.327\n",
      "Epoch 480 | train loss 0.327\n",
      "Epoch 490 | train loss 0.327\n",
      "Epoch 500 | train loss 0.327\n",
      "Epoch 510 | train loss 0.327\n",
      "Epoch 520 | train loss 0.327\n",
      "Epoch 530 | train loss 0.326\n",
      "Epoch 540 | train loss 0.326\n",
      "Epoch 550 | train loss 0.326\n",
      "Epoch 560 | train loss 0.326\n",
      "Epoch 570 | train loss 0.326\n",
      "Epoch 580 | train loss 0.326\n",
      "Epoch 590 | train loss 0.326\n",
      "Epoch 600 | train loss 0.326\n",
      "Epoch 610 | train loss 0.326\n",
      "Epoch 620 | train loss 0.326\n",
      "Epoch 630 | train loss 0.326\n",
      "Epoch 640 | train loss 0.325\n",
      "Epoch 650 | train loss 0.325\n",
      "Epoch 660 | train loss 0.325\n",
      "Epoch 670 | train loss 0.325\n",
      "Epoch 680 | train loss 0.325\n",
      "Epoch 690 | train loss 0.325\n",
      "Epoch 700 | train loss 0.324\n",
      "Epoch 710 | train loss 0.324\n",
      "Epoch 720 | train loss 0.324\n",
      "Epoch 730 | train loss 0.324\n",
      "Epoch 740 | train loss 0.324\n",
      "Epoch 750 | train loss 0.324\n",
      "Epoch 760 | train loss 0.324\n",
      "Epoch 770 | train loss 0.323\n",
      "Epoch 780 | train loss 0.323\n",
      "Epoch 790 | train loss 0.323\n",
      "Epoch 800 | train loss 0.323\n",
      "Epoch 810 | train loss 0.323\n",
      "Epoch 820 | train loss 0.322\n",
      "Epoch 830 | train loss 0.322\n",
      "Epoch 840 | train loss 0.322\n",
      "Epoch 850 | train loss 0.322\n",
      "Epoch 860 | train loss 0.322\n",
      "Epoch 870 | train loss 0.321\n",
      "Epoch 880 | train loss 0.321\n",
      "Epoch 890 | train loss 0.321\n",
      "Epoch 900 | train loss 0.321\n",
      "Epoch 910 | train loss 0.321\n",
      "Epoch 920 | train loss 0.321\n",
      "Epoch 930 | train loss 0.321\n",
      "Epoch 940 | train loss 0.320\n",
      "Epoch 950 | train loss 0.320\n",
      "Epoch 960 | train loss 0.320\n",
      "Epoch 970 | train loss 0.320\n",
      "Epoch 980 | train loss 0.319\n",
      "Epoch 990 | train loss 0.319\n",
      "Epoch 1000 | train loss 0.319\n",
      "test loss 0.312\n",
      "Epoch 10 | train loss 1.905\n",
      "Epoch 20 | train loss 0.413\n",
      "Epoch 30 | train loss 0.373\n",
      "Epoch 40 | train loss 0.368\n",
      "Epoch 50 | train loss 0.365\n",
      "Epoch 60 | train loss 0.363\n",
      "Epoch 70 | train loss 0.360\n",
      "Epoch 80 | train loss 0.359\n",
      "Epoch 90 | train loss 0.358\n",
      "Epoch 100 | train loss 0.356\n",
      "Epoch 110 | train loss 0.355\n",
      "Epoch 120 | train loss 0.355\n",
      "Epoch 130 | train loss 0.354\n",
      "Epoch 140 | train loss 0.353\n",
      "Epoch 150 | train loss 0.352\n",
      "Epoch 160 | train loss 0.351\n",
      "Epoch 170 | train loss 0.350\n",
      "Epoch 180 | train loss 0.349\n",
      "Epoch 190 | train loss 0.349\n",
      "Epoch 200 | train loss 0.348\n",
      "Epoch 210 | train loss 0.347\n",
      "Epoch 220 | train loss 0.346\n",
      "Epoch 230 | train loss 0.346\n",
      "Epoch 240 | train loss 0.345\n",
      "Epoch 250 | train loss 0.345\n",
      "Epoch 260 | train loss 0.345\n",
      "Epoch 270 | train loss 0.344\n",
      "Epoch 280 | train loss 0.344\n",
      "Epoch 290 | train loss 0.343\n",
      "Epoch 300 | train loss 0.343\n",
      "Epoch 310 | train loss 0.343\n",
      "Epoch 320 | train loss 0.342\n",
      "Epoch 330 | train loss 0.342\n",
      "Epoch 340 | train loss 0.341\n",
      "Epoch 350 | train loss 0.340\n",
      "Epoch 360 | train loss 0.340\n",
      "Epoch 370 | train loss 0.340\n",
      "Epoch 380 | train loss 0.340\n",
      "Epoch 390 | train loss 0.339\n",
      "Epoch 400 | train loss 0.338\n",
      "Epoch 410 | train loss 0.338\n",
      "Epoch 420 | train loss 0.338\n",
      "Epoch 430 | train loss 0.338\n",
      "Epoch 440 | train loss 0.337\n",
      "Epoch 450 | train loss 0.337\n",
      "Epoch 460 | train loss 0.337\n",
      "Epoch 470 | train loss 0.337\n",
      "Epoch 480 | train loss 0.337\n",
      "Epoch 490 | train loss 0.336\n",
      "Epoch 500 | train loss 0.336\n",
      "Epoch 510 | train loss 0.336\n",
      "Epoch 520 | train loss 0.335\n",
      "Epoch 530 | train loss 0.335\n",
      "Epoch 540 | train loss 0.335\n",
      "Epoch 550 | train loss 0.335\n",
      "Epoch 560 | train loss 0.334\n",
      "Epoch 570 | train loss 0.334\n",
      "Epoch 580 | train loss 0.334\n",
      "Epoch 590 | train loss 0.333\n",
      "Epoch 600 | train loss 0.333\n",
      "Epoch 610 | train loss 0.333\n",
      "Epoch 620 | train loss 0.332\n",
      "Epoch 630 | train loss 0.332\n",
      "Epoch 640 | train loss 0.332\n",
      "Epoch 650 | train loss 0.331\n",
      "Epoch 660 | train loss 0.331\n",
      "Epoch 670 | train loss 0.331\n",
      "Epoch 680 | train loss 0.330\n",
      "Epoch 690 | train loss 0.330\n",
      "Epoch 700 | train loss 0.329\n",
      "Epoch 710 | train loss 0.329\n",
      "Epoch 720 | train loss 0.329\n",
      "Epoch 730 | train loss 0.328\n",
      "Epoch 740 | train loss 0.328\n",
      "Epoch 750 | train loss 0.328\n",
      "Epoch 760 | train loss 0.327\n",
      "Epoch 770 | train loss 0.327\n",
      "Epoch 780 | train loss 0.327\n",
      "Epoch 790 | train loss 0.327\n",
      "Epoch 800 | train loss 0.326\n",
      "Epoch 810 | train loss 0.326\n",
      "Epoch 820 | train loss 0.326\n",
      "Epoch 830 | train loss 0.325\n",
      "Epoch 840 | train loss 0.325\n",
      "Epoch 850 | train loss 0.325\n",
      "Epoch 860 | train loss 0.324\n",
      "Epoch 870 | train loss 0.324\n",
      "Epoch 880 | train loss 0.324\n",
      "Epoch 890 | train loss 0.324\n",
      "Epoch 900 | train loss 0.323\n",
      "Epoch 910 | train loss 0.323\n",
      "Epoch 920 | train loss 0.323\n",
      "Epoch 930 | train loss 0.323\n",
      "Epoch 940 | train loss 0.322\n",
      "Epoch 950 | train loss 0.322\n",
      "Epoch 960 | train loss 0.322\n",
      "Epoch 970 | train loss 0.322\n",
      "Epoch 980 | train loss 0.322\n",
      "Epoch 990 | train loss 0.321\n",
      "Epoch 1000 | train loss 0.321\n",
      "test loss 0.307\n",
      "Epoch 10 | train loss 1.488\n",
      "Epoch 20 | train loss 0.416\n",
      "Epoch 30 | train loss 0.401\n",
      "Epoch 40 | train loss 0.397\n",
      "Epoch 50 | train loss 0.394\n",
      "Epoch 60 | train loss 0.392\n",
      "Epoch 70 | train loss 0.390\n",
      "Epoch 80 | train loss 0.389\n",
      "Epoch 90 | train loss 0.388\n",
      "Epoch 100 | train loss 0.386\n",
      "Epoch 110 | train loss 0.385\n",
      "Epoch 120 | train loss 0.384\n",
      "Epoch 130 | train loss 0.383\n",
      "Epoch 140 | train loss 0.383\n",
      "Epoch 150 | train loss 0.382\n",
      "Epoch 160 | train loss 0.381\n",
      "Epoch 170 | train loss 0.380\n",
      "Epoch 180 | train loss 0.379\n",
      "Epoch 190 | train loss 0.379\n",
      "Epoch 200 | train loss 0.378\n",
      "Epoch 210 | train loss 0.378\n",
      "Epoch 220 | train loss 0.377\n",
      "Epoch 230 | train loss 0.377\n",
      "Epoch 240 | train loss 0.377\n",
      "Epoch 250 | train loss 0.376\n",
      "Epoch 260 | train loss 0.376\n",
      "Epoch 270 | train loss 0.376\n",
      "Epoch 280 | train loss 0.376\n",
      "Epoch 290 | train loss 0.375\n",
      "Epoch 300 | train loss 0.375\n",
      "Epoch 310 | train loss 0.375\n",
      "Epoch 320 | train loss 0.375\n",
      "Epoch 330 | train loss 0.374\n",
      "Epoch 340 | train loss 0.374\n",
      "Epoch 350 | train loss 0.374\n",
      "Epoch 360 | train loss 0.374\n",
      "Epoch 370 | train loss 0.373\n",
      "Epoch 380 | train loss 0.373\n",
      "Epoch 390 | train loss 0.373\n",
      "Epoch 400 | train loss 0.373\n",
      "Epoch 410 | train loss 0.372\n",
      "Epoch 420 | train loss 0.372\n",
      "Epoch 430 | train loss 0.371\n",
      "Epoch 440 | train loss 0.371\n",
      "Epoch 450 | train loss 0.371\n",
      "Epoch 460 | train loss 0.371\n",
      "Epoch 470 | train loss 0.370\n",
      "Epoch 480 | train loss 0.370\n",
      "Epoch 490 | train loss 0.369\n",
      "Epoch 500 | train loss 0.369\n",
      "Epoch 510 | train loss 0.369\n",
      "Epoch 520 | train loss 0.368\n",
      "Epoch 530 | train loss 0.368\n",
      "Epoch 540 | train loss 0.368\n",
      "Epoch 550 | train loss 0.367\n",
      "Epoch 560 | train loss 0.367\n",
      "Epoch 570 | train loss 0.366\n",
      "Epoch 580 | train loss 0.366\n",
      "Epoch 590 | train loss 0.366\n",
      "Epoch 600 | train loss 0.365\n",
      "Epoch 610 | train loss 0.365\n",
      "Epoch 620 | train loss 0.364\n",
      "Epoch 630 | train loss 0.364\n",
      "Epoch 640 | train loss 0.363\n",
      "Epoch 650 | train loss 0.363\n",
      "Epoch 660 | train loss 0.362\n",
      "Epoch 670 | train loss 0.361\n",
      "Epoch 680 | train loss 0.361\n",
      "Epoch 690 | train loss 0.360\n",
      "Epoch 700 | train loss 0.360\n",
      "Epoch 710 | train loss 0.359\n",
      "Epoch 720 | train loss 0.359\n",
      "Epoch 730 | train loss 0.357\n",
      "Epoch 740 | train loss 0.357\n",
      "Epoch 750 | train loss 0.356\n",
      "Epoch 760 | train loss 0.356\n",
      "Epoch 770 | train loss 0.355\n",
      "Epoch 780 | train loss 0.355\n",
      "Epoch 790 | train loss 0.354\n",
      "Epoch 800 | train loss 0.353\n",
      "Epoch 810 | train loss 0.352\n",
      "Epoch 820 | train loss 0.351\n",
      "Epoch 830 | train loss 0.350\n",
      "Epoch 840 | train loss 0.350\n",
      "Epoch 850 | train loss 0.349\n",
      "Epoch 860 | train loss 0.349\n",
      "Epoch 870 | train loss 0.348\n",
      "Epoch 880 | train loss 0.347\n",
      "Epoch 890 | train loss 0.347\n",
      "Epoch 900 | train loss 0.346\n",
      "Epoch 910 | train loss 0.345\n",
      "Epoch 920 | train loss 0.344\n",
      "Epoch 930 | train loss 0.344\n",
      "Epoch 940 | train loss 0.343\n",
      "Epoch 950 | train loss 0.343\n",
      "Epoch 960 | train loss 0.342\n",
      "Epoch 970 | train loss 0.341\n",
      "Epoch 980 | train loss 0.340\n",
      "Epoch 990 | train loss 0.339\n",
      "Epoch 1000 | train loss 0.338\n",
      "test loss 0.310\n",
      "Epoch 10 | train loss 1.188\n",
      "Epoch 20 | train loss 0.415\n",
      "Epoch 30 | train loss 0.406\n",
      "Epoch 40 | train loss 0.402\n",
      "Epoch 50 | train loss 0.399\n",
      "Epoch 60 | train loss 0.398\n",
      "Epoch 70 | train loss 0.396\n",
      "Epoch 80 | train loss 0.395\n",
      "Epoch 90 | train loss 0.393\n",
      "Epoch 100 | train loss 0.392\n",
      "Epoch 110 | train loss 0.390\n",
      "Epoch 120 | train loss 0.389\n",
      "Epoch 130 | train loss 0.388\n",
      "Epoch 140 | train loss 0.387\n",
      "Epoch 150 | train loss 0.386\n",
      "Epoch 160 | train loss 0.385\n",
      "Epoch 170 | train loss 0.384\n",
      "Epoch 180 | train loss 0.383\n",
      "Epoch 190 | train loss 0.382\n",
      "Epoch 200 | train loss 0.380\n",
      "Epoch 210 | train loss 0.379\n",
      "Epoch 220 | train loss 0.378\n",
      "Epoch 230 | train loss 0.376\n",
      "Epoch 240 | train loss 0.375\n",
      "Epoch 250 | train loss 0.373\n",
      "Epoch 260 | train loss 0.372\n",
      "Epoch 270 | train loss 0.371\n",
      "Epoch 280 | train loss 0.370\n",
      "Epoch 290 | train loss 0.369\n",
      "Epoch 300 | train loss 0.368\n",
      "Epoch 310 | train loss 0.367\n",
      "Epoch 320 | train loss 0.366\n",
      "Epoch 330 | train loss 0.364\n",
      "Epoch 340 | train loss 0.363\n",
      "Epoch 350 | train loss 0.361\n",
      "Epoch 360 | train loss 0.360\n",
      "Epoch 370 | train loss 0.360\n",
      "Epoch 380 | train loss 0.359\n",
      "Epoch 390 | train loss 0.358\n",
      "Epoch 400 | train loss 0.357\n",
      "Epoch 410 | train loss 0.356\n",
      "Epoch 420 | train loss 0.355\n",
      "Epoch 430 | train loss 0.355\n",
      "Epoch 440 | train loss 0.354\n",
      "Epoch 450 | train loss 0.353\n",
      "Epoch 460 | train loss 0.353\n",
      "Epoch 470 | train loss 0.352\n",
      "Epoch 480 | train loss 0.352\n",
      "Epoch 490 | train loss 0.350\n",
      "Epoch 500 | train loss 0.349\n",
      "Epoch 510 | train loss 0.349\n",
      "Epoch 520 | train loss 0.348\n",
      "Epoch 530 | train loss 0.347\n",
      "Epoch 540 | train loss 0.346\n",
      "Epoch 550 | train loss 0.345\n",
      "Epoch 560 | train loss 0.344\n",
      "Epoch 570 | train loss 0.344\n",
      "Epoch 580 | train loss 0.343\n",
      "Epoch 590 | train loss 0.342\n",
      "Epoch 600 | train loss 0.341\n",
      "Epoch 610 | train loss 0.340\n",
      "Epoch 620 | train loss 0.340\n",
      "Epoch 630 | train loss 0.339\n",
      "Epoch 640 | train loss 0.338\n",
      "Epoch 650 | train loss 0.338\n",
      "Epoch 660 | train loss 0.337\n",
      "Epoch 670 | train loss 0.336\n",
      "Epoch 680 | train loss 0.335\n",
      "Epoch 690 | train loss 0.335\n",
      "Epoch 700 | train loss 0.334\n",
      "Epoch 710 | train loss 0.333\n",
      "Epoch 720 | train loss 0.333\n",
      "Epoch 730 | train loss 0.332\n",
      "Epoch 740 | train loss 0.331\n",
      "Epoch 750 | train loss 0.330\n",
      "Epoch 760 | train loss 0.329\n",
      "Epoch 770 | train loss 0.329\n",
      "Epoch 780 | train loss 0.328\n",
      "Epoch 790 | train loss 0.328\n",
      "Epoch 800 | train loss 0.328\n",
      "Epoch 810 | train loss 0.327\n",
      "Epoch 820 | train loss 0.327\n",
      "Epoch 830 | train loss 0.326\n",
      "Epoch 840 | train loss 0.326\n",
      "Epoch 850 | train loss 0.325\n",
      "Epoch 860 | train loss 0.325\n",
      "Epoch 870 | train loss 0.324\n",
      "Epoch 880 | train loss 0.324\n",
      "Epoch 890 | train loss 0.323\n",
      "Epoch 900 | train loss 0.323\n",
      "Epoch 910 | train loss 0.323\n",
      "Epoch 920 | train loss 0.322\n",
      "Epoch 930 | train loss 0.322\n",
      "Epoch 940 | train loss 0.321\n",
      "Epoch 950 | train loss 0.321\n",
      "Epoch 960 | train loss 0.320\n",
      "Epoch 970 | train loss 0.320\n",
      "Epoch 980 | train loss 0.319\n",
      "Epoch 990 | train loss 0.319\n",
      "Epoch 1000 | train loss 0.318\n",
      "test loss 0.309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_size</th>\n",
       "      <th>last_train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.2871752</td>\n",
       "      <td>0.28284174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.31930673</td>\n",
       "      <td>0.31212807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.32132322</td>\n",
       "      <td>0.30682832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.33827838</td>\n",
       "      <td>0.30968025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.31849787</td>\n",
       "      <td>0.30858234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emb_size last_train_loss   test_loss\n",
       "0       10       0.2871752  0.28284174\n",
       "0       50      0.31930673  0.31212807\n",
       "0      100      0.32132322  0.30682832\n",
       "0      150      0.33827838  0.30968025\n",
       "0      200      0.31849787  0.30858234"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_sizes = [10, 50, 100, 150, 200]\n",
    "\n",
    "result = pd.DataFrame(columns=['emb_size', 'last_train_loss', 'test_loss'])\n",
    "\n",
    "for emb_size in emb_sizes:\n",
    "    recsys_bias_model = RecSys_bias(num_reviewers, num_stores, emb_size=emb_size)\n",
    "    last_train_loss, test_loss = train_recsys(recsys_bias_model, train_data, test_data, epochs=1000, lr=0.01, weight_decay=1e-5, device=device)\n",
    "    model_result = pd.DataFrame({'emb_size': [emb_size], 'last_train_loss': [last_train_loss.detach().numpy()], 'test_loss': [test_loss.detach().numpy()]})\n",
    "    result = pd.concat([result, model_result], axis=0)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that smaller model performs better than larger model. Also, the test loss is similar to the train loss, suggesting that there may not be overfitting. In summary, our best performing model is on average having an absolute error of 0.28 out of a rating from 0-5. This represents an accuracy rate of  94.4%.\n",
    "\n",
    "Now, let's use all the data to train the model with emb_size=10 and make predictions on unseen reviewer-store pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss 2.890\n",
      "Epoch 20 | train loss 2.365\n",
      "Epoch 30 | train loss 1.837\n",
      "Epoch 40 | train loss 1.287\n",
      "Epoch 50 | train loss 0.775\n",
      "Epoch 60 | train loss 0.383\n",
      "Epoch 70 | train loss 0.341\n",
      "Epoch 80 | train loss 0.331\n",
      "Epoch 90 | train loss 0.328\n",
      "Epoch 100 | train loss 0.327\n",
      "Epoch 110 | train loss 0.325\n",
      "Epoch 120 | train loss 0.324\n",
      "Epoch 130 | train loss 0.323\n",
      "Epoch 140 | train loss 0.322\n",
      "Epoch 150 | train loss 0.322\n",
      "Epoch 160 | train loss 0.321\n",
      "Epoch 170 | train loss 0.320\n",
      "Epoch 180 | train loss 0.320\n",
      "Epoch 190 | train loss 0.319\n",
      "Epoch 200 | train loss 0.319\n",
      "Epoch 210 | train loss 0.318\n",
      "Epoch 220 | train loss 0.318\n",
      "Epoch 230 | train loss 0.318\n",
      "Epoch 240 | train loss 0.318\n",
      "Epoch 250 | train loss 0.317\n",
      "Epoch 260 | train loss 0.317\n",
      "Epoch 270 | train loss 0.317\n",
      "Epoch 280 | train loss 0.317\n",
      "Epoch 290 | train loss 0.317\n",
      "Epoch 300 | train loss 0.317\n",
      "Epoch 310 | train loss 0.317\n",
      "Epoch 320 | train loss 0.317\n",
      "Epoch 330 | train loss 0.317\n",
      "Epoch 340 | train loss 0.317\n",
      "Epoch 350 | train loss 0.316\n",
      "Epoch 360 | train loss 0.316\n",
      "Epoch 370 | train loss 0.316\n",
      "Epoch 380 | train loss 0.316\n",
      "Epoch 390 | train loss 0.316\n",
      "Epoch 400 | train loss 0.316\n",
      "Epoch 410 | train loss 0.316\n",
      "Epoch 420 | train loss 0.316\n",
      "Epoch 430 | train loss 0.316\n",
      "Epoch 440 | train loss 0.316\n",
      "Epoch 450 | train loss 0.316\n",
      "Epoch 460 | train loss 0.316\n",
      "Epoch 470 | train loss 0.316\n",
      "Epoch 480 | train loss 0.316\n",
      "Epoch 490 | train loss 0.316\n",
      "Epoch 500 | train loss 0.316\n",
      "Epoch 510 | train loss 0.315\n",
      "Epoch 520 | train loss 0.315\n",
      "Epoch 530 | train loss 0.315\n",
      "Epoch 540 | train loss 0.315\n",
      "Epoch 550 | train loss 0.315\n",
      "Epoch 560 | train loss 0.315\n",
      "Epoch 570 | train loss 0.315\n",
      "Epoch 580 | train loss 0.315\n",
      "Epoch 590 | train loss 0.315\n",
      "Epoch 600 | train loss 0.315\n",
      "Epoch 610 | train loss 0.315\n",
      "Epoch 620 | train loss 0.315\n",
      "Epoch 630 | train loss 0.315\n",
      "Epoch 640 | train loss 0.315\n",
      "Epoch 650 | train loss 0.315\n",
      "Epoch 660 | train loss 0.314\n",
      "Epoch 670 | train loss 0.314\n",
      "Epoch 680 | train loss 0.314\n",
      "Epoch 690 | train loss 0.314\n",
      "Epoch 700 | train loss 0.314\n",
      "Epoch 710 | train loss 0.314\n",
      "Epoch 720 | train loss 0.314\n",
      "Epoch 730 | train loss 0.314\n",
      "Epoch 740 | train loss 0.314\n",
      "Epoch 750 | train loss 0.314\n",
      "Epoch 760 | train loss 0.314\n",
      "Epoch 770 | train loss 0.314\n",
      "Epoch 780 | train loss 0.314\n",
      "Epoch 790 | train loss 0.314\n",
      "Epoch 800 | train loss 0.314\n",
      "Epoch 810 | train loss 0.314\n",
      "Epoch 820 | train loss 0.314\n",
      "Epoch 830 | train loss 0.314\n",
      "Epoch 840 | train loss 0.314\n",
      "Epoch 850 | train loss 0.314\n",
      "Epoch 860 | train loss 0.314\n",
      "Epoch 870 | train loss 0.314\n",
      "Epoch 880 | train loss 0.314\n",
      "Epoch 890 | train loss 0.314\n",
      "Epoch 900 | train loss 0.314\n",
      "Epoch 910 | train loss 0.313\n",
      "Epoch 920 | train loss 0.313\n",
      "Epoch 930 | train loss 0.313\n",
      "Epoch 940 | train loss 0.313\n",
      "Epoch 950 | train loss 0.313\n",
      "Epoch 960 | train loss 0.313\n",
      "Epoch 970 | train loss 0.313\n",
      "Epoch 980 | train loss 0.313\n",
      "Epoch 990 | train loss 0.313\n",
      "Epoch 1000 | train loss 0.313\n",
      "Epoch 1010 | train loss 0.313\n",
      "Epoch 1020 | train loss 0.313\n",
      "Epoch 1030 | train loss 0.313\n",
      "Epoch 1040 | train loss 0.313\n",
      "Epoch 1050 | train loss 0.313\n",
      "Epoch 1060 | train loss 0.313\n",
      "Epoch 1070 | train loss 0.313\n",
      "Epoch 1080 | train loss 0.313\n",
      "Epoch 1090 | train loss 0.313\n",
      "Epoch 1100 | train loss 0.313\n",
      "Epoch 1110 | train loss 0.313\n",
      "Epoch 1120 | train loss 0.313\n",
      "Epoch 1130 | train loss 0.313\n",
      "Epoch 1140 | train loss 0.313\n",
      "Epoch 1150 | train loss 0.312\n",
      "Epoch 1160 | train loss 0.312\n",
      "Epoch 1170 | train loss 0.312\n",
      "Epoch 1180 | train loss 0.312\n",
      "Epoch 1190 | train loss 0.312\n",
      "Epoch 1200 | train loss 0.312\n",
      "Epoch 1210 | train loss 0.312\n",
      "Epoch 1220 | train loss 0.312\n",
      "Epoch 1230 | train loss 0.312\n",
      "Epoch 1240 | train loss 0.312\n",
      "Epoch 1250 | train loss 0.312\n",
      "Epoch 1260 | train loss 0.312\n",
      "Epoch 1270 | train loss 0.312\n",
      "Epoch 1280 | train loss 0.312\n",
      "Epoch 1290 | train loss 0.312\n",
      "Epoch 1300 | train loss 0.312\n",
      "Epoch 1310 | train loss 0.312\n",
      "Epoch 1320 | train loss 0.312\n",
      "Epoch 1330 | train loss 0.312\n",
      "Epoch 1340 | train loss 0.312\n",
      "Epoch 1350 | train loss 0.312\n",
      "Epoch 1360 | train loss 0.312\n",
      "Epoch 1370 | train loss 0.312\n",
      "Epoch 1380 | train loss 0.312\n",
      "Epoch 1390 | train loss 0.312\n",
      "Epoch 1400 | train loss 0.312\n",
      "Epoch 1410 | train loss 0.312\n",
      "Epoch 1420 | train loss 0.312\n",
      "Epoch 1430 | train loss 0.312\n",
      "Epoch 1440 | train loss 0.312\n",
      "Epoch 1450 | train loss 0.312\n",
      "Epoch 1460 | train loss 0.312\n",
      "Epoch 1470 | train loss 0.312\n",
      "Epoch 1480 | train loss 0.312\n",
      "Epoch 1490 | train loss 0.312\n",
      "Epoch 1500 | train loss 0.312\n",
      "test loss 0.309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3116, grad_fn=<MeanBackward0>), tensor(0.3092))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final model\n",
    "final_data = data.copy()\n",
    "\n",
    "# Encoding reviewerId with continous Id\n",
    "final_reviewerId = np.sort(np.unique(final_data.reviewerId.values))\n",
    "num_reviewers = len(final_reviewerId)\n",
    "reviewerid2idx = {o:i for i,o in enumerate(final_reviewerId)}\n",
    "final_data['reviewerId'] = final_data['reviewerId'].map(reviewerid2idx)\n",
    "\n",
    "# Encoding StoreId with continous Id\n",
    "final_storeId = np.sort(np.unique(final_data.storeId.values))\n",
    "num_stores = len(final_storeId)\n",
    "storeid2idx = {o:i for i,o in enumerate(final_storeId)}\n",
    "final_data['storeId'] = final_data['storeId'].map(storeid2idx)\n",
    "\n",
    "final_model = RecSys_bias(num_reviewers, num_stores, emb_size=10)\n",
    "train_recsys(final_model, final_data, final_data, epochs=1500, lr=0.01, weight_decay=1e-5,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the final model for our recommendation system. We can use this model to generate a list of new recommendations for each reviewer. First, we will generate a list of review-store pair that is currently not present in our dataset, then use our model to make predictions for those new pairs. Finally, we can shortlist the top n pairs with highest rating and convert the ```reviewerid``` and ```storeid``` back to original id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate table of all reviewer_store pair\n",
    "import itertools\n",
    "all_reviewers = [i for i in range(0,len(final_reviewerId))]\n",
    "all_stores = [j for j in range(0,len(final_storeId))]\n",
    "all_combinations = list(itertools.product(all_reviewers, all_stores))\n",
    "all_data = pd.DataFrame(all_combinations, columns=['reviewerId', 'storeId'])\n",
    "\n",
    "# Join all pairs with the existing data to get the real rating.\n",
    "all_data = pd.merge(all_data, final_data, how='left', on=['reviewerId', 'storeId'])\n",
    "all_data.rename(columns={\"overall\":\"rating\"}, inplace=True)\n",
    "\n",
    "# Extract reviewer-store pair with no existing rating\n",
    "new_pair = all_data[all_data['rating'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_22272\\642763186.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_pair['rating'] = predictions\n",
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_22272\\642763186.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_pair['rank'] = new_pair.groupby('reviewerId')['rating'].rank(method='dense', ascending=True).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerId</th>\n",
       "      <th>storeId</th>\n",
       "      <th>rating</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.426768</td>\n",
       "      <td>4734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.427008</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.926070</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.013690</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.391284</td>\n",
       "      <td>3948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.069200</td>\n",
       "      <td>2118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.408282</td>\n",
       "      <td>4245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.308590</td>\n",
       "      <td>3153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.391606</td>\n",
       "      <td>3953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.085932</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewerId  storeId    rating  rank\n",
       "0           0        0  2.426768  4734\n",
       "1           0        1  2.427008  4744\n",
       "2           0        2  0.926070   106\n",
       "3           0        3  2.013690  1959\n",
       "4           0        4  2.391284  3948\n",
       "5           0        5  2.069200  2118\n",
       "6           0        6  2.408282  4245\n",
       "7           0        7  2.308590  3153\n",
       "8           0        8  2.391606  3953\n",
       "9           0        9  1.085932   447"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for pairs with no existing rating\n",
    "\n",
    "reviewers = torch.LongTensor(new_pair.reviewerId.values)\n",
    "stores = torch.LongTensor(new_pair.storeId.values)\n",
    "predictions = final_model(reviewers, stores).detach().numpy()\n",
    "new_pair['rating'] = predictions\n",
    "\n",
    "# compute the rank of each store to each reviewer\n",
    "new_pair['rank'] = new_pair.groupby('reviewerId')['rating'].rank(method='dense', ascending=True).astype(int)\n",
    "new_pair.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_22272\\3678831406.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  suggestion['reviewerId_original'] = suggestion['reviewerId'].map(idx2reviewerid)\n",
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_22272\\3678831406.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  suggestion['storeId_original'] = suggestion['storeId'].map(idx2storeid)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerId</th>\n",
       "      <th>storeId</th>\n",
       "      <th>rating</th>\n",
       "      <th>rank</th>\n",
       "      <th>reviewerId_original</th>\n",
       "      <th>storeId_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>0.411366</td>\n",
       "      <td>2</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>bzra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1383</td>\n",
       "      <td>0.374666</td>\n",
       "      <td>1</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>k1vg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4605</td>\n",
       "      <td>0.411569</td>\n",
       "      <td>3</td>\n",
       "      <td>00006b05-61f8-4f5c-9244-00ba8d97bc0e</td>\n",
       "      <td>wpvu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "      <td>0.118365</td>\n",
       "      <td>3</td>\n",
       "      <td>00017c85-e20e-4d9c-8d52-c224bf2b7d6f</td>\n",
       "      <td>bzra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4605</td>\n",
       "      <td>0.099286</td>\n",
       "      <td>1</td>\n",
       "      <td>00017c85-e20e-4d9c-8d52-c224bf2b7d6f</td>\n",
       "      <td>wpvu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98704</th>\n",
       "      <td>32901</td>\n",
       "      <td>4605</td>\n",
       "      <td>0.324824</td>\n",
       "      <td>2</td>\n",
       "      <td>z9vfu4jf</td>\n",
       "      <td>wpvu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98705</th>\n",
       "      <td>32901</td>\n",
       "      <td>4653</td>\n",
       "      <td>0.327629</td>\n",
       "      <td>3</td>\n",
       "      <td>z9vfu4jf</td>\n",
       "      <td>x17k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98706</th>\n",
       "      <td>32902</td>\n",
       "      <td>264</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>3</td>\n",
       "      <td>z9vyx9yi</td>\n",
       "      <td>bzra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98707</th>\n",
       "      <td>32902</td>\n",
       "      <td>4605</td>\n",
       "      <td>0.201193</td>\n",
       "      <td>1</td>\n",
       "      <td>z9vyx9yi</td>\n",
       "      <td>wpvu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98708</th>\n",
       "      <td>32902</td>\n",
       "      <td>4653</td>\n",
       "      <td>0.205706</td>\n",
       "      <td>2</td>\n",
       "      <td>z9vyx9yi</td>\n",
       "      <td>x17k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98709 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerId  storeId    rating  rank  \\\n",
       "0               0      264  0.411366     2   \n",
       "1               0     1383  0.374666     1   \n",
       "2               0     4605  0.411569     3   \n",
       "3               1      264  0.118365     3   \n",
       "4               1     4605  0.099286     1   \n",
       "...           ...      ...       ...   ...   \n",
       "98704       32901     4605  0.324824     2   \n",
       "98705       32901     4653  0.327629     3   \n",
       "98706       32902      264  0.215512     3   \n",
       "98707       32902     4605  0.201193     1   \n",
       "98708       32902     4653  0.205706     2   \n",
       "\n",
       "                        reviewerId_original storeId_original  \n",
       "0      00006b05-61f8-4f5c-9244-00ba8d97bc0e             bzra  \n",
       "1      00006b05-61f8-4f5c-9244-00ba8d97bc0e             k1vg  \n",
       "2      00006b05-61f8-4f5c-9244-00ba8d97bc0e             wpvu  \n",
       "3      00017c85-e20e-4d9c-8d52-c224bf2b7d6f             bzra  \n",
       "4      00017c85-e20e-4d9c-8d52-c224bf2b7d6f             wpvu  \n",
       "...                                     ...              ...  \n",
       "98704                              z9vfu4jf             wpvu  \n",
       "98705                              z9vfu4jf             x17k  \n",
       "98706                              z9vyx9yi             bzra  \n",
       "98707                              z9vyx9yi             wpvu  \n",
       "98708                              z9vyx9yi             x17k  \n",
       "\n",
       "[98709 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suggest the top n stores to each reviewer\n",
    "n = 3\n",
    "suggestion = new_pair[new_pair['rank'] <=n]\n",
    "\n",
    "# Convert the reviewId and storeId back to original code\n",
    "idx2reviewerid = {i:o for i,o in enumerate(final_reviewerId)}\n",
    "idx2storeid = {i:o for i,o in enumerate(final_storeId)}\n",
    "\n",
    "suggestion['reviewerId_original'] = suggestion['reviewerId'].map(idx2reviewerid)\n",
    "suggestion['storeId_original'] = suggestion['storeId'].map(idx2storeid)\n",
    "suggestion.reset_index(drop=True, inplace=True)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Saving the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models\\Recommendation_system_for_food_deliveries_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create model directory path\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True,\n",
    "                 exist_ok=True)\n",
    "\n",
    "# Create model save\n",
    "MODEL_NAME = \"Recommendation_system_for_food_deliveries_state_dict.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=final_model.state_dict(),\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models\\Recommendation_system_for_food_deliveries_full_model.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create model directory path\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True,\n",
    "                 exist_ok=True)\n",
    "\n",
    "# Create model save\n",
    "MODEL_NAME = \"Recommendation_system_for_food_deliveries_full_model.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=final_model,\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecSys_bias(\n",
       "  (reviewers_emb): Embedding(32903, 10)\n",
       "  (stores_emb): Embedding(5221, 10)\n",
       "  (reviewers_bias): Embedding(32903, 1)\n",
       "  (stores_bias): Embedding(5221, 1)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new instance\n",
    "loaded_model = RecSys_bias(num_reviewers, num_stores, emb_size=10)\n",
    "\n",
    "# Load in the save state_dict()\n",
    "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "\n",
    "# Send the model to the target device\n",
    "loaded_model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
